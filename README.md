# The Invisible Harms of Security Tech

The technology industry presents security as a tradeoff between harm and *inconvenience*, implying that nobody is meaningfully harmed when security backfires. This framing supports the position that more security is always better, and that those unwilling to adopt ever-stricter security are irrational and irresponsible.

Yet security technologies do backfire, and when they do the harms can be devastating and irreparable. When authentication systems backfire, locking us out of our online accounts, we can lose decades of records, photos, correspondence, contacts, and other precious digital assets. The same is true for encrypted file systems. When our phone's security features lock us out of navigation while we are driving on the highway, demanding that we take action to unlock it, we can be put at risk of physical harm.[^locked-navigation] When security systems prevent us from doing our job, they can interfere with our livelihood. As we become ever more reliant on technology, losing access to it even briefly can cause harm; imagine being unable to access medical data before emergency treatment, being blocked from reaching out to those you trust when you need them most, or being rendered unreachable from those who need you most. 

<!-- Losses:
Lost human connection and relationships -->

Though nearly all of us have suffered harm when security technologies backfire, collectively causing what must be many millions of lifetimes of lost memories, contacts and other records, these aggregate harms and others are almost entirely invisible to society. Since losses occur one person at a time, few incidents are large enough to be newsworthy.[^cryptocurrency] Most of us don't report or speak up when the security measures meant to protect us backfires because we blame ourselves. While companies are required to report when security fails to keep attackers out, they are not required to report when security fails to let us in. So, when we hear about security failures, we only hear about breaches, ransomware, and other stories that reinforce the narrative that more security is always better.

Even experts can only guess at the scale of harms and data lost because they go unreported and uncounted. We can at best infer the scale is vast because, anecdotally, stories of loss are sadly ubiquitous when one starts asking.

Those skeptical that such large harms could hide in plain site will find ample historical precedent. At the start of the industrial revolution, occupational deaths and injuries were extremely common, but were rarely reported. The technological industrialists of the time blamed these ‘accidents‘ on the victims (workers), employers were not required to report them, and the media did not consider the individual deaths newsworthy, so they went uncounted. In 1891, the first year the US government tallied railroad worker injuries, they reported 2,660 fatalities and 26,140 other injuries.[^railroad-injuries] As the automotive era dawned in the 20th century, the deaths and injuries resulting from automotive crashes were also initially unreported and uncounted. The technological industrialists of the time blamed these harms everyone but the cars they produced: initially drivers, until the bogeyman of the jaywalker was invented.[^no-accidents] 

Perhaps the best analog for security tech that of medicines. As a society, we acknowledge that medicines can save some lives while ending others; that they are often-essential and yet potentially hazardous. Hence, we test medicine before making it widely available, track side effects during and after testing, require certain medications be sold only with a written prescription of an certified expert (a doctor) who can be held accountable for malpractice, and disclose side effects to both those prescribing and using these products. No such requirements exist for security technologies.

Because the designers of tech products may be held accountable for security failures, but not backfires, they are likely to over-prescribe security and under-protect against it backfiring. They will make our digital locks less forgiving than we might choose were we able to know the actual tradeoffs involved.

For example, Apple's engineers designed their KeyChain to [permanently lock out users after ten incorrect guesses of their iCloud security code](https://support.apple.com/guide/security/escrow-security-for-icloud-keychain-sec3e341e75d/1/web/1). This design allows Apple to promise that even their engineers cannot access the data, even under government order. This design decision may benefit users who face a high risk from government eavesdropping, like journalists or political dissidents, but may be harmful to those for whom the risk of losing access outweighs those benefits. Whereas Apple would be required to disclose if the data were breached, they don't publish the number of user accounts that become permanently locked out. Users cannot make the right choice because they are neither given a choice, nor the data they would need to make that choice.

Similarly, 1Password encrypts each of their users' online data with both that user's master password and an encryption key stored on every device the user has installed 1Password, which the user can also print as part of an emergency kit. If a user loses the devices on which 1Password is installed and didn't print an emergency kit, their data is permanently lost. To have a reputation for the best security, 1Password made a design decision that increases the chance of permanent data loss. They do not track or publish how often such loss happens. They don't even track the fraction of users who print an emergency kit.

End-to-end encrypted messaging apps like Signal and WhatsApp often fail to sync data for reasons that are often impossible to comprehend, such as if a device is offline for a period of time. Signal does disclose that “for your security, chat history isn't transferred to new linked devices.” If you upgrade your phone, your new phone won't have access to your chat history. Loss occurs by design, but the consequences of that design choice are not measured.

To address the harms when security technologies like these backfire, we must treat these hazards like other hazards: We must ensure they are reported and counted. We need researchers to measure the prevalence of harms and their impact, and aggregate those losses over the affected populations. We need investigators to continue to track losses longitudinally (over time) even though such research is harder to fund, publish, and attract media attention to, than research that identifies security vulnerabilities. We should demand that tech companies track and report the harms when security backfires, as they are required to do when security is breached. We might even want to require certain classes of security tech to be independently tested before deployment as we do for medicines and other hazardous products.


[^locked-navigation]: This example is not hypothetical or dated. The security features in my 2023 phone have silently removed the Google Maps app's access to my location while it was navigating me on a highway, and locked the app entirely during other drives. This backfire occurred even though the navigation app (Google Maps), and car integration system (Android Auto) were all made by the same organization as the phone's operating system (Google's Android), which are presumably tested to work together.

[^cryptocurrency]: Notable exceptions are when cryptocurrency holders lose the keys to millions of dollars in assets.

[^railroad-injuries]: See [US Census Bureau Historical Statistics](https://www2.census.gov/library/publications/1960/compendia/hist_stats_colonial-1957/hist_stats_colonial-1957-chQ.pdf) Q41-152.

[^no-accidents]: For a history of deaths and injuries from industrial work and automotive crashes were once uncounted and invisible, I recommend [Jessie Singer](https://jessiesinger.com/)'s 2022 book [“There Are No Accidents”](https://www.simonandschuster.com/books/There-Are-No-Accidents/Jessie-Singer/9781797139241).